{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1580c945",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0686613",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from config import config\n",
    "from data_processing import (\n",
    "    load_and_process_csv,\n",
    "    load_documents_from_csv,\n",
    "    split_documents,\n",
    ")\n",
    "from embeddings import initialize_embeddings\n",
    "from evaluation import (\n",
    "    RAGEvaluator,\n",
    "    evaluate_rag_pipeline,\n",
    "    generate_test_dataset,\n",
    ")\n",
    "from rag_chain import RAGOutput\n",
    "from retrieval import build_retriever\n",
    "from utils import print_separator, setup_environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f8f342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pubmed_scraper import PubMedScraper\n",
    "\n",
    "# scraper = PubMedScraper(email = \"olandechris@gmail.com\")\n",
    "\n",
    "# data = scraper.search_with_llm(query = \"Find me 50 papers about Covid 19 from 2019 to 2025\")\n",
    "\n",
    "# df = scraper.search_with_llm(query = \"Find papers about the impact of Gaza war on children\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d636b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(input_csv: str, output_csv: str | Path | None = None) -> Path:\n",
    "    \"\"\"\n",
    "    Prepare and process the input data.\n",
    "\n",
    "    Args:\n",
    "        input_csv: Path to input CSV file\n",
    "        output_csv: Path to save processed CSV (optional)\n",
    "\n",
    "    Returns:\n",
    "        Path to the processed CSV file\n",
    "    \"\"\"\n",
    "    print_separator(\"DATA PREPARATION\")\n",
    "    if output_csv is None:\n",
    "        output_csv = Path(config.paths.data_dir) / \"tests.csv\"\n",
    "\n",
    "    # Load and process CSV\n",
    "    print(f\"Loading data from {input_csv}\")\n",
    "    df = load_and_process_csv(input_csv, output_csv)\n",
    "\n",
    "    return Path(output_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rag_system(csv_path: str):\n",
    "    \"\"\"\n",
    "    Build the complete RAG system.\n",
    "\n",
    "    Args:\n",
    "        csv_path: Path to processed CSV file\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (rag_chain, splitted_documents, embeddings)\n",
    "    \"\"\"\n",
    "    print_separator(\"BUILDING RAG SYSTEM\")\n",
    "    print(\"Initializing Embeddings ...\")\n",
    "\n",
    "    embeddings = initialize_embeddings(\n",
    "        model_name=config.model.embedding_model,\n",
    "        cache_dir=config.model.embedding_cache_dir,\n",
    "    )\n",
    "\n",
    "    # Load and split the documents\n",
    "    print(\"Loading documents...\")\n",
    "    documents = load_documents_from_csv(csv_path)\n",
    "\n",
    "    print(\"Splitting documents...\")\n",
    "    splitted_documents = split_documents(documents, embeddings)\n",
    "    print(f\"Created {len(splitted_documents)} document chunks\")\n",
    "\n",
    "    # Build Retriever\n",
    "    print(\"Building retriever\")\n",
    "    retriever = build_retriever(splitted_documents, embeddings, config.retriever)\n",
    "\n",
    "    # Initialize RAG Chain\n",
    "    print(\"Initializing RAG Chain ...\")\n",
    "\n",
    "    # Initialize RAG chain\n",
    "    print(\"Initializing RAG chain...\")\n",
    "    rag_chain = RAGOutput(\n",
    "        prompt_name=\"rlm/rag-prompt\",\n",
    "        retriever=retriever,\n",
    "        llm_model=config.model.deepseek_model,\n",
    "    )\n",
    "    rag_chain.create_chain()\n",
    "\n",
    "    print(\"RAG system built successfully\")\n",
    "    return rag_chain, splitted_documents, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acae54e37e7d407bbb7b55eff062a284",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(\n",
    "    rag_chain: RAGOutput,\n",
    "    test_dataset_path: str | Path | None = None,\n",
    "    results_path: str | Path | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Run evaluation on the RAG system.\n",
    "\n",
    "    Args:\n",
    "        rag_chain: Configured RAG chain\n",
    "        test_dataset_path: Path to test dataset (optional)\n",
    "        results_path: Path to save results (optional)\n",
    "    \"\"\"\n",
    "    print_separator(\"EVALUATION\")\n",
    "\n",
    "    if test_dataset_path is None:\n",
    "        test_dataset_path = config.paths.rag_eval_dir / \"generated_testset.csv\"\n",
    "\n",
    "    if results_path is None:\n",
    "        results_path = config.paths.rag_eval_dir / \"results_deepseek_fastembed.csv\"\n",
    "\n",
    "    # Run evaluation\n",
    "    results = evaluate_rag_pipeline(\n",
    "        rag_chain=rag_chain,\n",
    "        input_csv_path=str(test_dataset_path),\n",
    "        output_csv_path=str(results_path),\n",
    "        question_column=\"user_input\",\n",
    "    )\n",
    "\n",
    "    print(f\"Evaluation complete. Results shape: {results.shape}\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a63283cbaf04dbcab1f6479b197f3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_testset(\n",
    "    splitted_documents, embeddings, llm, testset_size: int = 10\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate synthetic test dataset using RAGAS.\n",
    "\n",
    "    Args:\n",
    "        splitted_documents: Split documents\n",
    "        embeddings: Embeddings model\n",
    "        llm: Language model\n",
    "        testset_size: Number of test samples\n",
    "\n",
    "    Returns:\n",
    "        Generated test dataset as DataFrame\n",
    "    \"\"\"\n",
    "    print_separator(\"GENERATING TEST DATASET\")\n",
    "\n",
    "    output_path = config.paths.rag_eval_dir / \"generated_testset.csv\"\n",
    "\n",
    "    dataset = generate_test_dataset(\n",
    "        documents=splitted_documents,\n",
    "        embeddings=embeddings,\n",
    "        llm=llm,\n",
    "        testset_size=testset_size,\n",
    "        output_path=str(output_path),\n",
    "    )\n",
    "\n",
    "    print(f\"Test dataset generated with {len(dataset)} samples\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6669371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full_evaluation(embeddings):\n",
    "    \"\"\"\n",
    "    Run comprehensive evaluation using RAGEvaluator.\n",
    "\n",
    "    Args:\n",
    "        embeddings: Embeddings model to use for evaluation\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of evaluation results for all model pairs\n",
    "    \"\"\"\n",
    "    print_separator(\"COMPREHENSIVE EVALUATION\")\n",
    "\n",
    "    # Initialize evaluator\n",
    "    evaluator = RAGEvaluator(\n",
    "        max_workers=1,\n",
    "        timeout=180,\n",
    "        generative_models=[\"deepseek-chat\"],\n",
    "        embedding_models=[\"fastembed\"],\n",
    "    )\n",
    "\n",
    "    # Run evaluation for all model pairs\n",
    "    results = evaluator.evaluate_all_models(\n",
    "        evaluation_embeddings=embeddings, results_dir=str(config.paths.rag_eval_dir)\n",
    "    )\n",
    "\n",
    "    # Print summary\n",
    "    print_separator(\"EVALUATION SUMMARY\")\n",
    "    for model_pair, df in results.items():\n",
    "        print(f\"\\n{model_pair}:\")\n",
    "        print(f\"  Shape: {df.shape}\")\n",
    "        if len(df) > 0:\n",
    "            metric_cols = [\n",
    "                col\n",
    "                for col in df.columns\n",
    "                if col\n",
    "                not in [\"user_input\", \"reference\", \"response\", \"retrieved_contexts\"]\n",
    "            ]\n",
    "            if metric_cols:\n",
    "                print(f\"  Metrics: {', '.join(metric_cols)}\")\n",
    "                for col in metric_cols:\n",
    "                    if df[col].dtype in [\"float64\", \"int64\"]:\n",
    "                        print(f\"    {col}: {df[col].mean():.4f} (avg)\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0aa19c3",
   "metadata": {},
   "source": [
    "# Program Entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f720ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup environment\n",
    "setup_environment()\n",
    "\n",
    "# Initialize configuration\n",
    "print_separator(\"INITIALIZING\")\n",
    "print(f\"Data directory: {config.paths.data_dir}\")\n",
    "print(f\"RAG evaluation directory: {config.paths.rag_eval_dir}\")\n",
    "\n",
    "llm = config.initialize_llm()\n",
    "dspy_lm = config.initialize_dspy()\n",
    "# Prepare data\n",
    "csv_path = prepare_data(\"data/gaza_war_impact_children.csv\")\n",
    "\n",
    "# Build RAG system\n",
    "rag_chain, splitted_documents, embeddings = build_rag_system(str(csv_path))\n",
    "\n",
    "# Generate test dataset\n",
    "# generate_synthetic_testset(splitted_documents, embeddings, llm, testset_size=10)\n",
    "\n",
    "# Run evaluation\n",
    "results = run_evaluation(rag_chain)\n",
    "\n",
    "# eval_results = run_full_evaluation(embeddings)"
   ]
  }
 ],
 "metadata": {
  "checkpoint": {
   "checkpointId": "notebookcheckpoint_918",
   "eventId": 918,
   "timestamp": 1759841544768
  },
  "kernelspec": {
   "display_name": "PyCharmProjects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "vincent": {
   "sessionId": "cfc305bccd64ee55a62be711_2025-10-05T15-22-51-228Z"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
