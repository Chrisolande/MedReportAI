{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1580c945",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0686613",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from config import config\n",
    "from data_processing import (\n",
    "    load_and_process_csv,\n",
    "    load_documents_from_csv,\n",
    "    split_documents,\n",
    ")\n",
    "from embeddings import initialize_embeddings\n",
    "from evaluation import evaluate_rag_pipeline, generate_test_dataset\n",
    "from rag_chain import RAGOutput\n",
    "from retrieval import build_retriever\n",
    "from utils import print_separator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f8f342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pubmed_scraper import PubMedScraper\n",
    "\n",
    "# scraper = PubMedScraper(email = \"olandechris@gmail.com\")\n",
    "\n",
    "# data = scraper.search_with_llm(query = \"Find me 50 papers about Covid 19 from 2019 to 2025\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d636b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(input_csv: str, output_csv: str | Path | None = None) -> Path:\n",
    "    \"\"\"\n",
    "    Prepare and process the input data.\n",
    "\n",
    "    Args:\n",
    "        input_csv: Path to input CSV file\n",
    "        output_csv: Path to save processed CSV (optional)\n",
    "\n",
    "    Returns:\n",
    "        Path to the processed CSV file\n",
    "    \"\"\"\n",
    "    print_separator(\"DATA PREPARATION\")\n",
    "    if output_csv is None:\n",
    "        output_csv = Path(config.paths.data_dir) / \"tests.csv\"\n",
    "\n",
    "    # Load and process CSV\n",
    "    print(f\"Loading data from {input_csv}\")\n",
    "    df = load_and_process_csv(input_csv, output_csv)\n",
    "\n",
    "    return Path(output_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rag_system(csv_path: str):\n",
    "    \"\"\"\n",
    "    Build the complete RAG system.\n",
    "\n",
    "    Args:\n",
    "        csv_path: Path to processed CSV file\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (rag_chain, splitted_documents, embeddings)\n",
    "    \"\"\"\n",
    "    print_separator(\"BUILDING RAG SYSTEM\")\n",
    "    print(\"Initializing Embeddings ...\")\n",
    "\n",
    "    embeddings = initialize_embeddings(\n",
    "        model_name=config.model.embedding_model,\n",
    "        cache_dir=config.model.embedding_cache_dir,\n",
    "    )\n",
    "\n",
    "    # Load and split the documents\n",
    "    print(\"Loading documents...\")\n",
    "    documents = load_documents_from_csv(csv_path)\n",
    "\n",
    "    print(\"Splitting documents...\")\n",
    "    splitted_documents = split_documents(documents, embeddings)\n",
    "    print(f\"Created {len(splitted_documents)} document chunks\")\n",
    "\n",
    "    # Build Retriever\n",
    "    print(\"Building retriever\")\n",
    "    retriever = build_retriever(splitted_documents, embeddings, config.retriever)\n",
    "\n",
    "    # Initialize RAG Chain\n",
    "    print(\"Initializing RAG Chain ...\")\n",
    "\n",
    "    # Initialize RAG chain\n",
    "    print(\"Initializing RAG chain...\")\n",
    "    rag_chain = RAGOutput(\n",
    "        prompt_name=\"rlm/rag-prompt\",\n",
    "        retriever=retriever,\n",
    "        llm_model=config.model.deepseek_model,\n",
    "    )\n",
    "    rag_chain.create_chain()\n",
    "\n",
    "    print(\"RAG system built successfully\")\n",
    "    return rag_chain, splitted_documents, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acae54e37e7d407bbb7b55eff062a284",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(\n",
    "    rag_chain: RAGOutput,\n",
    "    test_dataset_path: str | Path | None = None,\n",
    "    results_path: str | Path | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Run evaluation on the RAG system.\n",
    "\n",
    "    Args:\n",
    "        rag_chain: Configured RAG chain\n",
    "        test_dataset_path: Path to test dataset (optional)\n",
    "        results_path: Path to save results (optional)\n",
    "    \"\"\"\n",
    "    print_separator(\"EVALUATION\")\n",
    "\n",
    "    if test_dataset_path is None:\n",
    "        test_dataset_path = config.paths.rag_eval_dir / \"generated_testset.csv\"\n",
    "\n",
    "    if results_path is None:\n",
    "        results_path = config.paths.rag_eval_dir / \"results_deepseek_fastembed.csv\"\n",
    "\n",
    "    # Run evaluation\n",
    "    results = evaluate_rag_pipeline(\n",
    "        rag_chain=rag_chain,\n",
    "        input_csv_path=str(test_dataset_path),\n",
    "        output_csv_path=str(results_path),\n",
    "        question_column=\"user_input\",\n",
    "    )\n",
    "\n",
    "    print(f\"Evaluation complete. Results shape: {results.shape}\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a63283cbaf04dbcab1f6479b197f3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_testset(\n",
    "    splitted_documents, embeddings, llm, testset_size: int = 10\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate synthetic test dataset using RAGAS.\n",
    "\n",
    "    Args:\n",
    "        splitted_documents: Split documents\n",
    "        embeddings: Embeddings model\n",
    "        llm: Language model\n",
    "        testset_size: Number of test samples\n",
    "\n",
    "    Returns:\n",
    "        Generated test dataset as DataFrame\n",
    "    \"\"\"\n",
    "    print_separator(\"GENERATING TEST DATASET\")\n",
    "\n",
    "    output_path = config.paths.rag_eval_dir / \"generated_testset.csv\"\n",
    "\n",
    "    dataset = generate_test_dataset(\n",
    "        documents=splitted_documents,\n",
    "        embeddings=embeddings,\n",
    "        llm=llm,\n",
    "        testset_size=testset_size,\n",
    "        output_path=str(output_path),\n",
    "    )\n",
    "\n",
    "    print(f\"Test dataset generated with {len(dataset)} samples\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6669371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = scraper.search_with_llm(query = \"Find papers about the impact of Gaza war on children\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdc8c89c7104fffa095e18ddfef8986",
   "metadata": {},
   "source": [
    "## Evaluate the RAG - Add the section to the evaluation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938c804e27f84196a10c8828c723f798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation_llm = LangchainLLMWrapper(llm)\n",
    "# evaluation_embeddings = LangchainEmbeddingsWrapper(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504fb2a444614c0babb325280ed9130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @dataclass\n",
    "# class RAGEvaluator:\n",
    "#     \"\"\"RAG Evaluation pipeline for model-embedding pairs.\"\"\"\n",
    "\n",
    "#     max_workers: int = 1\n",
    "#     timeout: int = 180\n",
    "#     generative_models: list[str] = field(default_factory=lambda: [\"deepseek-chat\"])\n",
    "#     embedding_models: list[str] = field(default_factory=lambda: [\"fastembed\"])\n",
    "#     metrics: list = field(\n",
    "#         default_factory=lambda: [\n",
    "#             ContextRecall(),\n",
    "#             ContextPrecision(),\n",
    "#             AnswerSimilarity(),\n",
    "#             ContextEntityRecall(),\n",
    "#             NoiseSensitivity(),\n",
    "#             Faithfulness(),\n",
    "#         ]\n",
    "#     )\n",
    "\n",
    "#     def __post_init__(self):\n",
    "#         \"\"\"Initialize RunConfig after dataclass initialization.\"\"\"\n",
    "#         self.run_config = RunConfig(max_workers=self.max_workers, timeout=self.timeout)\n",
    "\n",
    "#     def parse_contexts(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "#         \"\"\"Parse retrieved_contexts from string to list.\"\"\"\n",
    "#         if \"retrieved_contexts\" in data.columns:\n",
    "#             data[\"retrieved_contexts\"] = data[\"retrieved_contexts\"].apply(\n",
    "#                 ast.literal_eval\n",
    "#             )\n",
    "#         return data\n",
    "\n",
    "#     def prepare_dataset(self, data: pd.DataFrame) -> EvaluationDataset:\n",
    "#         \"\"\"Prepare evaluation dataset from dataframe.\"\"\"\n",
    "#         eval_data = data[\n",
    "#             [\"user_input\", \"reference\", \"response\", \"retrieved_contexts\"]\n",
    "#         ].to_dict(orient=\"records\")\n",
    "#         return EvaluationDataset.from_list(eval_data)\n",
    "\n",
    "#     def run_evaluation(\n",
    "#         self, input_csv_path: str, evaluation_embeddings\n",
    "#     ) -> pd.DataFrame:\n",
    "#         \"\"\"\n",
    "#         Run evaluation on input data.\n",
    "\n",
    "#         Args:\n",
    "#             input_csv_path: Path to input CSV file\n",
    "#             evaluation_embeddings: Embeddings to use for evaluation\n",
    "\n",
    "#         Returns:\n",
    "#             DataFrame with evaluation results\n",
    "#         \"\"\"\n",
    "#         data = pd.read_csv(input_csv_path)\n",
    "#         data = self.parse_contexts(data)\n",
    "#         eval_dataset = self.prepare_dataset(data)\n",
    "\n",
    "#         evaluator_llm = LangchainLLMWrapper(ChatDeepSeek(model=\"deepseek-chat\"))\n",
    "\n",
    "#         results = evaluate(\n",
    "#             dataset=eval_dataset,\n",
    "#             metrics=self.metrics,\n",
    "#             llm=evaluator_llm,\n",
    "#             embeddings=evaluation_embeddings,\n",
    "#             run_config=self.run_config,\n",
    "#         )\n",
    "\n",
    "#         return results.to_pandas()\n",
    "\n",
    "#     def evaluate_all_models(self, evaluation_embeddings):\n",
    "#         \"\"\"\n",
    "#         Evaluate all model-embedding pairs.\n",
    "\n",
    "#         Args:\n",
    "#             evaluation_embeddings: Embeddings to use for evaluation\n",
    "#         \"\"\"\n",
    "#         for model, embedding in zip(self.generative_models, self.embedding_models):\n",
    "#             model_pair = f\"{model}_{embedding}\"\n",
    "#             output_csv_path = f\"RAGEvaluation/evaluation_results_{model_pair}.csv\"\n",
    "\n",
    "#             if os.path.exists(output_csv_path):\n",
    "#                 print(f\"Loading existing results for {model_pair}\")\n",
    "#                 df = pd.read_csv(output_csv_path)\n",
    "#             else:\n",
    "#                 print(f\"Running evaluation for {model_pair}\")\n",
    "#                 input_csv_path = \"RAGEvaluation/results_deepseek_fastembed.csv\"\n",
    "\n",
    "#                 df = self.run_evaluation(input_csv_path, evaluation_embeddings)\n",
    "#                 df.to_csv(output_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bbdb311c014d738909a11f9e486628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluator = RAGEvaluator(max_workers=1, timeout=180)\n",
    "# evaluator.evaluate_all_models(evaluation_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43b363d81ae4b689946ece5c682cd59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"<1-hop>\\n\\nArticle: Family systems approach to attachment relations, war trauma, and mental health among Palestinian children and parents.<b>Background</b>: Trauma affects the family unit as a whole; however, most existing research uses individual or, at most, dyadic approaches to analyse families with histories of trauma. <b>Objective</b>: This study aims to identify potentially distinct family types according to attachment, parenting, and sibling relations, to analyse how these family types differ with respect to war trauma, and to explore how children's mental health and cognitive processing differ across these family types. <b>Method:</b> Participants included Palestinian mothers and fathers (<i>N</i>\\xa0=\\xa0325) and their children (one per family; 49.4% girls; 10-13\\xa0years old; mean\\xa0±\\xa0<i>SD</i> age\\xa0=\\xa011.35 ± 0.57 years) after the Gaza War of 2008-2009. Both parents reported their exposure to war trauma, secure attachment availability, and parenting practices, as well as the target child's internalizing and externalizing symptoms [Strengths and Difficulties Questionnaire (SDQ)]. Children reported their symptoms of post-traumatic stress disorder (on the Children's Revised Impact Event Scale), depression (Birleson), and SDQ, as well as their post-traumatic cognitions (Children's Post Traumatic Cognitions Inventory). <b>Results:</b> A cluster analysis identified four family types. The largest type reflected secure attachment and optimal relationships (security and positive family relationships, 36.2%, <i>n</i>\\xa0=\\xa0102), and the smallest exhibited insecurity and problematic relationships (insecurity and negative family relationships, 15.6%; <i>n</i>\\xa0=\\xa044). Further, families with discrepant experiences (23.0%; <i>n</i>\\xa0=\\xa065) and moderate security and neutral relationships (25.2%; <i>n</i>\\xa0=\\xa071) emerged.\",\n",
       " \"<2-hop>\\n\\nArticle: The impact of occupation on child health in a Palestinian refugee camp.This article focuses on child health in the Palestinian refugee camp of Dheisheh in the West Bank region of the Occupied Palestinian Territories. Thirty in-depth interviews were carried out with parents to determine their perceptions of their children's health. The questions related to physical, mental and social well-being, access to health facilities, factors that were likely to hinder health and measures that could be implemented to improve child health. The study was carried out prior to and during the Gaza War in December 2008 that resulted in the deaths of 1380 Palestinians including 431 children and 112 women [1]. The effects of occupation, conflict and being a refugee had a detrimental impact on perceptions of health.\",\n",
       " \"<3-hop>\\n\\nMothers reported infants' language, fine- and gross-motor, and socioemotional skills at T2 and researchers tested infants' motor, cognitive-language and socioemotional skills using the Bayley Scales of Infant development (BSID-II) at T3. Mothers reported their mental health problems (symptoms of post-traumatic stress disorder [PTSD], depression and somatization) at T2 and T3 as well as dyadic interaction quality (the emotional availability self-report, [EA-SR] brief) at T2. First, the structural equation model (SEM) on direct effects indicated, in contrast to our hypotheses, that maternal prenatal exposure to traumatic war events did not associate with infants' developmental skills at T2 and predicted higher level of developmental skills at T3. Second, as hypothesized, we found two negative underlying mechanisms (paths) between high exposure and low levels of motor, cognitive-language, and socioemotional skills at T3: (1) through increased maternal mental health problems at T2, which then were associated with problems at T3, and (2) through increased maternal mental health problems at T2, which then were associated with a low quality of mother-infant-interaction and low level of infant developmental skills at T2. Improving maternal mental health and encouraging close and positive dyadic interaction can be critical for infant sensorimotor, cognitive, and socioemotional development in war conditions.\"]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = pd.read_csv(\"RAGEvaluation/results_deepseek_fastembed.csv\")"
   ]
  }
 ],
 "metadata": {
  "checkpoint": {
   "checkpointId": "notebookcheckpoint_918",
   "eventId": 918,
   "timestamp": 1759841544768
  },
  "kernelspec": {
   "display_name": "PyCharmProjects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "vincent": {
   "sessionId": "cfc305bccd64ee55a62be711_2025-10-05T15-22-51-228Z"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
